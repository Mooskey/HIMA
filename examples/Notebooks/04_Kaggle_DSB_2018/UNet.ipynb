{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e61ef2d8-f315-4f7f-b07e-1de0f4e8441a",
    "_uuid": "1677fddbb95f7545b6540e9201f3339a0fdbfc5d"
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "This a python 3 notebook on how to start segmenting nuclei using a neural network.\n",
    "\n",
    "* Uses Keras and Tensorflow\n",
    "* Modified fork of the [Keras U-Net starter notebook](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277)\n",
    "* Using: [U-Net](https://arxiv.org/abs/1505.04597) architecture\n",
    "* U-Net architecture is commonly used for image segmentation problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependent libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c332549b-8d23-4bb5-8497-e7a8eb8b21d2",
    "_uuid": "5c38504af3a84bee68c66d3cde74443c58df422f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import types\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from keras.models import Model, load_model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers.core import Dropout, Lambda\n",
    "    from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "    from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
    "    from keras.layers.merge import concatenate\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "    from keras import backend as K\n",
    "    from keras.metrics import binary_crossentropy\n",
    "    from keras.models import model_from_json\n",
    "except:\n",
    "    print (\"Install Keras 2 (cmd: $sudo pip3 install keras) to run this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize some global parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "DEFAULT_UNIT_SIZE = 64\n",
    "DEFAULT_DROPOUT = 0.85\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 1024\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = types.SimpleNamespace()\n",
    "args.data_path = ['./data/']\n",
    "args.config_file = ['./model/trained_2018_03_20-21_19_02_config_UNet.json']\n",
    "args.weights_file = ['./model/trained_2018_03_20-21_19_02_weights_UNet.model']\n",
    "args.output_dir = ['./model/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer_savepath = os.path.join(args.output_dir[0]     + \\\n",
    "                                     'checkpoint/UNet_I' + \\\n",
    "                                     str(IMG_WIDTH)  + '_'  + \\\n",
    "                                     str(IMG_HEIGHT) + '_'  + \\\n",
    "                                     'U' + str(DEFAULT_UNIT_SIZE)   + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(args.data_path[0]+'/train/')\n",
    "TEST_PATH = os.path.join(args.data_path[0]+'/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(args.data_path[0]+'train_aug/')\n",
    "print (TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch train and test IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ffa0caf0-2d1b-40f2-865b-8e6db88526b6",
    "_uuid": "3fb9d6530fbbd0e22e41fc4fd9fd9fc0bff027ac"
   },
   "outputs": [],
   "source": [
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "test_ids = next(os.walk(TEST_PATH))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59c4a25d-645f-4b74-9c53-145ac78cc481",
    "_uuid": "875af74f980236825de3a650825b46e25632422c"
   },
   "source": [
    "# Loading data:\n",
    "\n",
    "* Import all the images and associated masks. \n",
    "* Downsample both the training and test images to keep things light and manageable.\n",
    "* Store the record of the original sizes of the test images to upsample our predicted masks\n",
    "* Create correct run-length encodings later on using the upsampled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and resize training images and masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ca0cc34b-c26f-41ee-88d7-975aebdb634e",
    "_uuid": "9e389ba8bdb5b6fc03b231b6a6c84a8bde634053"
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[n] = img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "        mask_ = imread(path + '/masks/' + mask_file)\n",
    "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                      preserve_range=True), axis=-1)\n",
    "        mask = np.maximum(mask, mask_)\n",
    "    Y_train[n] = mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load augmented images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    if id_ != 'data':\n",
    "        path = TRAIN_PATH + id_\n",
    "        for file in os.listdir(path + '/images/'):\n",
    "            if file.endswith(\".png\"):\n",
    "                img_id = (os.path.join(path + '/images/' + str(file)))\n",
    "            else:\n",
    "                print (\"No image file found ...\")\n",
    "        try:\n",
    "            img = imread(img_id)[:,:,:IMG_CHANNELS]\n",
    "            img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "            X_train[n] = img\n",
    "        except:\n",
    "            print (\"Incompatible image dimensions detected. Resizing image: \" + str(img_id) +\" ...\")\n",
    "            try:\n",
    "                img = imread(img_id)[:,:]\n",
    "                stacked_img = np.stack((img,)*3, -1)\n",
    "                stacked_img = resize(stacked_img, (IMG_HEIGHT, IMG_WIDTH), \\\n",
    "                                               mode='constant', \\\n",
    "                                               preserve_range=True)\n",
    "            except:\n",
    "                print ('Failed to read from image: ' + str(img_id))\n",
    "                pass\n",
    "            X_train[n] = stacked_img\n",
    "        mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "        for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "            mask_ = imread(path + '/masks/' + mask_file)\n",
    "            mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                      preserve_range=True), axis=-1)\n",
    "            mask = np.maximum(mask, mask_)\n",
    "        Y_train[n] = mask\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and resize test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "sizes_test = []\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    if id_ != 'data':\n",
    "        path = TEST_PATH + id_\n",
    "        img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "        sizes_test.append([img.shape[0], img.shape[1]])\n",
    "        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "        X_test[n] = img\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training and test data to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(os.path.join(args.data_path[0]+'/train_aug_256.npz'), xtrain=X_train, ytrain=Y_train)\n",
    "np.savez_compressed(os.path.join(args.data_path[0]+'/test_256.npz'), xtest=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved data from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(args.data_path[0]+'/train_aug_256.npz'))\n",
    "X_train = train_data['xtrain']\n",
    "Y_train = train_data['ytrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(os.path.join(args.data_path[0]+'/test_256.npz'))\n",
    "X_test = test_data['xtest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running checks on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "88829b53-50ce-45d9-9540-77dd7384ad4c",
    "_uuid": "283af26f0860b7069bdfd133c746e5d20971542c"
   },
   "outputs": [],
   "source": [
    "ix = random.randint(0, len(train_ids))\n",
    "imshow(X_train[ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(Y_train[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2574ffe9-b911-4bfd-a00f-9ba5c25f45de",
    "_uuid": "938648da705689a0f940ff462477c801db3f0737"
   },
   "source": [
    "# Create evaluation metrics:\n",
    "\n",
    "Now we try to define the *mean average precision at different intersection over union (IoU) thresholds* metric in Keras. TensorFlow has a mean IoU metric, but it doesn't have any native support for the mean over multiple thresholds, so I tried to implement this. **I'm by no means certain that this implementation is correct, though!** Any assistance in verifying this would be most welcome! \n",
    "\n",
    "*Update: This implementation is most definitely not correct due to the very large discrepancy between the results reported here and the LB results. It also seems to just increase over time no matter what when you train ... *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle specific IoU metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1df6f3a-d58f-434b-9216-ef7be38637d4",
    "_uuid": "5abd38950ae99b60f8afec7656eb654a48d449fe"
   },
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU metric using tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_tf(y_true, y_pred):\n",
    "   score, up_opt = tf.metrics.mean_iou(y_true, y_pred, NUM_CLASSES)\n",
    "   K.get_session().run(tf.local_variables_initializer())\n",
    "   with tf.control_dependencies([up_opt]):\n",
    "       score = tf.identity(score)\n",
    "   return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICE Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_dice(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred)-K.log(dice_coef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dice = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3b9f148-1dba-4b6a-981b-6cdbf394fc3c",
    "_uuid": "986488a4c5223576be370e224426a30431911eb2"
   },
   "source": [
    "# Create a neural network model for image segmentation:\n",
    "\n",
    "### U-Net architecture overview:\n",
    "\n",
    "Next we build our U-Net model, loosely based on [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) and very similar to [this repo](https://github.com/jocicmarko/ultrasound-nerve-segmentation) from the Kaggle Ultrasound Nerve Segmentation competition.\n",
    "\n",
    "![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build U-Net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "DEFAULT_ACTIVATION = 'elu' # 'relu', 'elu'\n",
    "unit_size = DEFAULT_UNIT_SIZE\n",
    "dropout = DEFAULT_DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1dbc57c-b497-4ccb-b077-2053203ab7ed",
    "_uuid": "0aa97d66c29f45dfac9b0f45fcf74ba0e778ba5d"
   },
   "outputs": [],
   "source": [
    "def build_UNet(unit_size = None,\n",
    "                 final_max_pooling = None):\n",
    "    s = Lambda(lambda x: x / 255) (inputs)\n",
    "\n",
    "    c1 = Conv2D(unit_size, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (s)\n",
    "    c1 = Dropout(dropout) (c1)\n",
    "    c1 = Conv2D(unit_size, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = Conv2D(unit_size*2, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (p1)\n",
    "    c2 = Dropout(dropout) (c2)\n",
    "    c2 = Conv2D(unit_size*2, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = Conv2D(unit_size*4, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (p2)\n",
    "    c3 = Dropout(dropout) (c3)\n",
    "    c3 = Conv2D(unit_size*4, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = Conv2D(unit_size*8, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (p3)\n",
    "    c4 = Dropout(dropout) (c4)\n",
    "    c4 = Conv2D(unit_size*8, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c4)\n",
    "    p4 = MaxPooling2D((2, 2)) (c4)\n",
    "\n",
    "    c5 = Conv2D(unit_size*16, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (p4)\n",
    "    c5 = Dropout(dropout) (c5)\n",
    "    c5 = Conv2D(unit_size*16, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c5)\n",
    "    c5 = Dropout(dropout) (c5)\n",
    "    c5 = Conv2D(unit_size*16, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(unit_size*8, (2, 2), \n",
    "                         strides=(2, 2), \n",
    "                         padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(unit_size*8, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (u6)\n",
    "    c6 = Dropout(dropout) (c6)\n",
    "    c6 = Conv2D(unit_size*8, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(unit_size*4, (2, 2), \n",
    "                         strides=(2, 2), \n",
    "                         padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(unit_size*4, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (u7)\n",
    "    c7 = Dropout(dropout) (c7)\n",
    "    c7 = Conv2D(unit_size*4, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(unit_size*2, (2, 2), \n",
    "                         strides=(2, 2), \n",
    "                         padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(unit_size*2, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (u8)\n",
    "    c8 = Dropout(dropout) (c8)\n",
    "    c8 = Conv2D(unit_size*2, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(unit_size, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(unit_size, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (u9)\n",
    "    c9 = Dropout(dropout) (c9)\n",
    "    c9 = Conv2D(unit_size, (3, 3), \n",
    "                activation=DEFAULT_ACTIVATION, \n",
    "                kernel_initializer='he_normal', \n",
    "                padding='same') (c9)\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_UNet(unit_size = DEFAULT_UNIT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction_model(args):\n",
    "    try:\n",
    "        print (args.config_file[0]) \n",
    "        with open(args.config_file[0]) as json_file:\n",
    "              model_json = json_file.read()\n",
    "        model = model_from_json(model_json)\n",
    "        return model\n",
    "    except:\n",
    "        print (\"Please specify a model configuration file ...\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction_model_weights(args):\n",
    "    try:\n",
    "        model.load_weights(args.weights_file[0])\n",
    "        print (\"Loaded model weights from: \" + str(args.weights_file[0]))\n",
    "        return model\n",
    "        \n",
    "    except:\n",
    "        print (\"Error loading model weights ...\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_checkpoint = False\n",
    "load_from_config = True\n",
    "load_model_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model = None,\n",
    "                     checkpointer_savepath = None, \n",
    "                     args = None,\n",
    "                     mean_iou = None,\n",
    "                     mean_iou_tf = None,\n",
    "                     dice_coef = None,\n",
    "                     bce_dice = None,\n",
    "                     dice_coef_loss = None,\n",
    "                     load_from_checkpoint = None,\n",
    "                     load_from_config = None,\n",
    "                     load_model_weights = None):\n",
    "    if load_from_checkpoint == True:\n",
    "        if use_dice == True:\n",
    "            model = load_model(checkpointer_savepath,                  \\\n",
    "                               custom_objects={'mean_iou': mean_iou,   \\\n",
    "                                               'mean_iou_tf': mean_iou_tf,   \\\n",
    "                                               'dice_coef': dice_coef, \\\n",
    "                                               'bce_dice': bce_dice,   \\\n",
    "                                               'dice_coef_loss': dice_coef_loss})\n",
    "        else:\n",
    "            model = load_model(checkpointer_savepath, \\\n",
    "                               custom_objects={'mean_iou': mean_iou})\n",
    "    elif load_from_config == True:\n",
    "        model = load_prediction_model(args)\n",
    "        model = load_prediction_model_weights(args)\n",
    "    elif load_model_weights == True:\n",
    "        try:\n",
    "            model = load_prediction_model_weights(args)\n",
    "        except:\n",
    "            print (\"An exception has occurred, while loading model weights ...\")\n",
    "    else:\n",
    "        model = model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_saved_model(model = model,\n",
    "                         checkpointer_savepath = checkpointer_savepath, \n",
    "                         args = args,\n",
    "                         mean_iou = mean_iou,\n",
    "                         mean_iou_tf = mean_iou_tf,\n",
    "                         dice_coef = dice_coef,\n",
    "                         bce_dice = bce_dice,\n",
    "                         dice_coef_loss = dice_coef_loss,\n",
    "                         load_from_checkpoint = load_from_checkpoint,\n",
    "                         load_from_config = load_from_config,\n",
    "                         load_model_weights = load_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=1e-7, decay=0.5, momentum=1, nesterov=True)\n",
    "rms = RMSprop(lr=1e-7, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "ada = Adagrad(lr=1e-7, epsilon=1e-08, decay=0.0)\n",
    "adam = Adam(lr=1e-9, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    \n",
    "DEFAULT_OPTIMIZER = adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and generate summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dice = True\n",
    "use_dice_loss = True\n",
    "use_custom_iou = True\n",
    "if use_dice == True and use_dice_loss == False:\n",
    "    model.compile(optimizer = DEFAULT_OPTIMIZER, \n",
    "              loss = bce_dice, \n",
    "              metrics = ['binary_crossentropy', \n",
    "                         dice_coef, \n",
    "                         mean_iou.\n",
    "                         mean_iou_tf])\n",
    "elif use_dice_loss == True and use_dice == True :\n",
    "    model.compile(optimizer = DEFAULT_OPTIMIZER, \n",
    "                   loss = dice_coef_loss, \n",
    "                   metrics = [dice_coef, \n",
    "                              'acc', \n",
    "                              'mse'])\n",
    "elif use_custom_iou == True:\n",
    "    model.compile(optimizer = DEFAULT_OPTIMIZER, \n",
    "                   loss = 'binary_crossentropy', \n",
    "                   metrics = [mean_iou,\n",
    "                              mean_iou_tf,\n",
    "                              'acc', \n",
    "                              'mse'])\n",
    "else:\n",
    "    model.compile(optimizer=DEFAULT_OPTIMIZER, \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=[mean_iou_tf, \n",
    "                           'acc', \n",
    "                           'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = True\n",
    "if model_summary == True:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "import pydot \n",
    "import graphviz # apt-get install -y graphviz libgraphviz-dev \n",
    "from IPython.display import SVG \n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file= os.path.join(args.output_dir[0] + '/model.png')) \n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model:\n",
    "\n",
    "* Model architecture uses ELU units, \n",
    "* Additional dropout layers to manage over-fitting.\n",
    "* Fit the model on the training data, using a validation split:train split of 0.1. \n",
    "* Small batch size because due to small sized dataset. \n",
    "* Checkpointing and early stopping during training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9415b1c4-aa69-41b9-a1e3-d6053dbd4f64",
    "_uuid": "c060db22daa2abf12b28240cd81bbcbf1ce1bf87",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(checkpointer_savepath,\\\n",
    "                               verbose=1,\\\n",
    "                               save_best_only=True)\n",
    "results = model.fit(X_train, Y_train, validation_split=0.125, batch_size=64, epochs=10, \n",
    "                    callbacks=[earlystopper, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timestamp():\n",
    "    timestring = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    print (\"Time stamp generated: \"+timestring)\n",
    "    return timestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = generate_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(args, name, model):\n",
    "    file_loc = args.output_dir[0]\n",
    "    file_pointer = os.path.join(file_loc+\"//trained_\"+ timestr)\n",
    "    model.save_weights(os.path.join(file_pointer + \"_weights\"+str(name)+\".model\"))    \n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(file_pointer+\"_config\"+str(name)+\".json\"), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    print (\"Saved the trained model weights to: \" + \n",
    "           str(os.path.join(file_pointer + \"_weights\"+str(name)+\".model\")))\n",
    "    print (\"Saved the trained model configuration as a json file to: \" + \n",
    "    str(os.path.join(file_pointer+\"_config\"+str(name)+\".json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(args, '_UNet', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(checkpointer_savepath,\\\n",
    "                               verbose=1,\\\n",
    "                               save_best_only=True)\n",
    "results = model.fit(X_train, \n",
    "                    Y_train, \n",
    "                    validation_split=0.2, \n",
    "                    batch_size=64, \n",
    "                    epochs=10, \n",
    "                    callbacks=[earlystopper, \n",
    "                               checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Generate predictions:\n",
    "\n",
    "### Run predictions on train, validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
    "preds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_in_batches = False\n",
    "preds_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prediction(predict_in_batches = None,\n",
    "                   preds_batch_size = None,\n",
    "                   X_test = None,\n",
    "                   sample_size = None):\n",
    "    if sample_size != None:\n",
    "        sample_size = int(sample_size)\n",
    "    else:\n",
    "        sample_size = len(X_test)\n",
    "    preds_test = []\n",
    "    if predict_in_batches == True:\n",
    "        for i in range(sample_size):\n",
    "            try:\n",
    "                if i+preds_batch_size <= sample_size:\n",
    "                    preds_result = model.predict(X_test[i:i+preds_batch_size], \n",
    "                                                 verbose =1)\n",
    "                    i = i+preds_batch_size\n",
    "                    preds_test.append(preds_result)\n",
    "                    del preds_result\n",
    "                    gc.collect()\n",
    "                    print (\"Successfully generated predictions for: \" \n",
    "                           + str(i) \n",
    "                           + \" / \" \n",
    "                           + str(sample_size)\n",
    "                           + \" test images\")\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                print (\"Failed generating predictions for: \" \n",
    "                       + str(i) \n",
    "                       + \" / \" \n",
    "                       + str(sample_size)\n",
    "                       + \" test images\")\n",
    "    else:\n",
    "        preds_test = model.predict(X_test, \n",
    "                                   verbose=1)\n",
    "    return np.asarray(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_test = (gen_prediction(predict_in_batches = predict_in_batches,\n",
    "                            preds_batch_size = preds_batch_size,\n",
    "                            X_test = X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of upsampled test masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2daa48d5-ac98-4e18-af3f-a582baaa44f0",
    "_uuid": "f841760b4abca1a25cb750822f88268bd79bf2ce"
   },
   "outputs": [],
   "source": [
    "def gen_upsampled_data(input_data = None,\n",
    "                       data_size = None):\n",
    "    upsampled_data = []\n",
    "    for i in range(len(input_data)):\n",
    "        upsampled_data.append(resize(np.squeeze(input_data[i]), \n",
    "                                               (data_size[i][0], \n",
    "                                                data_size[i][1]), \n",
    "                                                mode='constant', \n",
    "                                                preserve_range=True))\n",
    "    return upsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_upsampled = gen_upsampled_data(input_data = preds_test_t,\n",
    "                                          data_size = sizes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual comparison of predictions with random training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "649248cd-a1fb-4da6-ade2-4bebad44bcab",
    "_uuid": "7e06242a50870e07a080064a4912b761775990fa"
   },
   "outputs": [],
   "source": [
    "ix = random.randint(0, len(preds_train_t))\n",
    "imshow(X_train[ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(Y_train[ix]))\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_train_t[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visual comparison of predictions against random validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4f66b75c-c694-41a1-8c91-34bb6595837b",
    "_uuid": "d4ccbb559375bc2777ffb692a20adc313159f2cc"
   },
   "outputs": [],
   "source": [
    "ix = random.randint(0, len(preds_val_t))\n",
    "imshow(X_train[int(X_train.shape[0]*0.9):][ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(Y_train[int(Y_train.shape[0]*0.9):][ix]))\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_val_t[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual output of test data predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = random.randint(0, len(preds_test_t)-1)\n",
    "imshow(X_test[ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_test_t[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6690535-b2e4-49ac-98d9-7191bfabfb6f",
    "_uuid": "6a34c98de7c6ae473f676a34fe7e099b46764eca"
   },
   "source": [
    "##### Encode and submit results:\n",
    "\n",
    "* To submit our results, the output needs run-length encoding. \n",
    "* This notebook uses run-length encoding function described [here](https://www.kaggle.com/rakhlin/fast-run-length-encoding-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run-length encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59a0af60-a7d7-41ef-a6fe-9e3c72defa07",
    "_uuid": "4f99c1bf852e82b60bd4f982ca0df293f712cdf0"
   },
   "outputs": [],
   "source": [
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_rles(x, cutoff=0.5):\n",
    "    lab_img = label(x > cutoff)\n",
    "    for i in range(1, lab_img.max() + 1):\n",
    "        yield rle_encoding(lab_img == i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31133f8c-3f40-4dff-8e1d-898d56672332",
    "_uuid": "2e07f6afc4787b068ba714428145dcb3951d718f"
   },
   "source": [
    "### Generate a complete submission:\n",
    "Iterate through test IDs to generate run-length encodings for each seperate mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22fe24a1-7659-4cc9-9d23-211f38e5b99f",
    "_uuid": "089587843ed6a3955fdcb9b23a6ec3bf5d703688"
   },
   "outputs": [],
   "source": [
    "new_test_ids = []\n",
    "rles = []\n",
    "for n, id_ in enumerate(test_ids):\n",
    "    rle = list(prob_to_rles(preds_test_upsampled[n], \n",
    "               cutoff = 0.0))\n",
    "    rles.extend(rle)\n",
    "    new_test_ids.extend([id_] * len(rle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ba0ee3a-cca0-4349-83f6-09a1ac6fcb44",
    "_uuid": "ba589f56f5be1e6886bc88f5bf9e7d0a408e4048"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ImageId'] = new_test_ids\n",
    "sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
    "sub.to_csv('./sub-dsbowl2018-REMUNet_20180320.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "222475b9-3171-461a-90f0-a820a6bd2634",
    "_uuid": "fb5e6f8cca872f1bd7036f6d9ac2ed2cab615536",
    "collapsed": true
   },
   "source": [
    "### Notes for improvements the results:\n",
    "\n",
    "* Adjust hyper-parameters, \n",
    "* Tweaking the architecture a little bit\n",
    "* Train the model for a longer duration with early stopping\n",
    "\n",
    "**Have fun!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 1)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
