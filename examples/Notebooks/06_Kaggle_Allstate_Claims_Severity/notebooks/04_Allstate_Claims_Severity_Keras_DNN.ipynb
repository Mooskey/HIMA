{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Allstate Claims Severity](https://goo.gl/1DwHVy) -- Predictions using machine learning:\n",
    "\n",
    "### Author: Dr. Rahul Remanan, CEO and  Chief Imagination Officer [Moad Computer](https://www.moad.computer)\n",
    "\n",
    "The [Allstate Corporation](https://en.wikipedia.org/wiki/Allstate) is the one of the largest insurance providers in the United States and one of the largest that is publicly held. The company also has personal lines insurance operations in Canada. Allstate was founded in 1931 as part of Sears, Roebuck and Co., and was spun off in 1993.[1](https://goo.gl/ce2JJ2) The company has had its headquarters in Northfield Township, Illinois, near Northbrook since 1967.[2](https://goo.gl/oX4kfZ),[3](https://goo.gl/mcTd3y)\n",
    "\n",
    "As part of Allstate's ongoing efforts to develop automated methods of predicting the cost, and hence severity, of claims, they releasd a claims severity assessment dataset on Kaggle.[4](https://goo.gl/1DwHVy) In this challenge, datascientists were invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. The goal of this challenge was to help aspiring competitors demonstrate their insight into better ways of predicting claims severity.\n",
    "\n",
    "We will be using this dataset to build a deep neural network model using  tensorflow and keras.\n",
    "\n",
    "Users can run this notebook in [Jomiraki Cloud AI instance, with a subscription](https://www.moad.computer/store/p24/Jomiraki_datasci) or in try it for free using [Google CoLab](https://goo.gl/mGu4bH).\n",
    "\n",
    "#### To launch this example notebook instance in Jomiraki:\n",
    "\n",
    "* 1) $ git clone https://github.com/rahulremanan/HIMA\n",
    "* 2) From Jupyter home page, navigate to /HIMA/examples/Notebooks/06_Kaggle_Allstate_Claims_Severity/notebooks/\n",
    "* 3) Launch the notebook with a python 3 kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 04 -- Deep neural network model using [Keras](https://keras.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training and test files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "print('training: ', df_train.shape)\n",
    "print('test: ', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing:\n",
    "\n",
    "Convert the structured data into numpy arrays and separate features/targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = df_train.as_matrix()\n",
    "training_targets = training_samples[:,-1]\n",
    "training_samples = training_samples[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = df_test.as_matrix()\n",
    "test_samples = test_samples[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution histogram of target values from training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hist_plot = False\n",
    "if gen_hist_plot:\n",
    "    plt.hist(training_targets[np.where(training_targets < 15000)], \n",
    "             bins = 200, \n",
    "             normed=True)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Target Value')\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.title('Distribution of target values from training set.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the labels of the categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "# [0:116]\n",
    "allLabels = np.concatenate( ( training_samples[:, 0:116].flat , test_samples[:, 0:116].flat ) )\n",
    "le.fit( allLabels )\n",
    "del allLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_labels = False\n",
    "if print_labels:\n",
    "    print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the labels to int values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colIndex in range(116):\n",
    "    training_samples[:, colIndex] = le.transform(training_samples[:, colIndex])\n",
    "    test_samples[:, colIndex] = le.transform( test_samples[:, colIndex] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a deep neural network:\n",
    "\n",
    "Use keras and tensorflow to build a deep neural network predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.7\n",
    "activation = 'elu' # 'relu' or 'elu'\n",
    "unit_size = 20\n",
    "input_dim = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(unit_size=None, dropout=None, activation=None, input_dim=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(unit_size*50, input_dim=input_dim, kernel_initializer='normal', activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*50, kernel_initializer='normal', activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*50, kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*50, kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*25, kernel_initializer='normal', activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*25, kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*25, kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size*5, kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size, kernel_initializer='normal', activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(unit_size, kernel_initializer='normal', activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build compile and summarize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dnn_model(unit_size=unit_size, dropout=dropout, activation=activation, input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating regression estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(training_samples)\n",
    "Y_train = training_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_saved_model = True\n",
    "filepath = '../model/Allstate_claims_severity_DNN.h5'\n",
    "\n",
    "if load_saved_model and os.path.exists(filepath):\n",
    "    model = keras.models.load_model(filepath)\n",
    "    print (\"Loaded saved model ...\")\n",
    "\n",
    "log = model.fit(X_train, Y_train, \n",
    "                epochs=2, \n",
    "                batch_size=50, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions formatted for Kaggle submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scale.transform(test_samples)\n",
    "pred_targets = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results to csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(df_test, columns=['id'])\n",
    "df_res['loss'] = pred_targets\n",
    "print(df_res.iloc[0])\n",
    "df_res.to_csv('DNN_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
